{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "aWlqv_f2p4ZX",
        "outputId": "3fa6f18f-d8df-4f7d-e78f-720e26509923"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'torch' has no attribute 'version' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3798c669501f>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistilBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistilBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mreplace_return_docstrings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 34\u001b[0;31m from .generic import (\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mContextManagers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_torch_pytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_model_output_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_pytree.Context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2476\u001b[0;31m from torch import (\n\u001b[0m\u001b[1;32m   2477\u001b[0m     \u001b[0mexport\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2478\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdynamic_shapes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConstraint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mShapesCollection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexported_program\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExportedProgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleCallEntry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleCallSignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraph_signature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExportBackwardSignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExportGraphSignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/export/dynamic_shapes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexported_program\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExportedProgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/export/exported_program.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_higher_order_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograd_not_implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_class_registry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFakeScriptObject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfirst_call_function_nn_module_stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_higher_order_ops/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_higher_order_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from torch._higher_order_ops.flex_attention import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mflex_attention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mflex_attention_backward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_higher_order_ops/cond.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDispatchKey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_subclasses/functional_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minductor_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_functionalization_reapply_views_tls\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_reapply_views\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# use fx aot graph codegen cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m fx_graph_cache = (\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TORCHINDUCTOR_FX_GRAPH_CACHE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_fbcode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/config.py\u001b[0m in \u001b[0;36mis_fbcode\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_fbcode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"git_version\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torch' has no attribute 'version' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize, ngrams\n",
        "from nltk.stem import PorterStemmer\n",
        "import os\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load English NLP pipeline for NER\n",
        "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Cyber terms mapping\n",
        "cyber_terms = {\n",
        "    'phishing': 'phish',\n",
        "    'hacked': 'hack',\n",
        "    'scammed': 'scam',\n",
        "    'fraudulent': 'fraud'\n",
        "}\n",
        "fraud_types = [\n",
        "    'financial_fraud',\n",
        "    'identity_theft',\n",
        "    'phishing_scam',\n",
        "    'ransomware_attack',\n",
        "    'social_media_fraud',\n",
        "    'cryptocurrency_scam',\n",
        "    'ecommerce_fraud',\n",
        "    'banking_fraud'\n",
        "]\n",
        "\n",
        "# Disable W&B logging (optional)\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load and inspect data\n",
        "data = pd.read_csv(\"train_clean.csv\")\n",
        "\n",
        "# Check for empty values in essential columns\n",
        "data = data.dropna(subset=[\"category\", \"crimeaditionalinfo\"])\n",
        "\n",
        "# Preprocessing: Remove stopwords, apply stemming, and extract features\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [ps.stem(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Feature Extraction\n",
        "def extract_features(text):\n",
        "    # Detect cyber terms\n",
        "    detected_terms = {term: name for term, name in cyber_terms.items() if term in text.lower()}\n",
        "\n",
        "    # Create feature flags\n",
        "    features = {\n",
        "        'contains_financial_terms': bool(re.search(r'bank|credit|upi|money|payment', text.lower())),\n",
        "        'contains_urgency': bool(re.search(r'urgent|immediate|quick|emergency', text.lower())),\n",
        "        'contains_threats': bool(re.search(r'threat|hack|stolen|compromise', text.lower())),\n",
        "        'message_length': len(text.split()),\n",
        "        'has_numbers': bool(re.search(r'\\d', text))\n",
        "    }\n",
        "\n",
        "    # NER for identifying entities like organizations and people\n",
        "    doc = spacy_nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # Bigram and Trigram Analysis\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    bigrams = list(ngrams(tokens, 2))\n",
        "    trigrams = list(ngrams(tokens, 3))\n",
        "\n",
        "    return detected_terms, features, entities, bigrams, trigrams\n",
        "\n",
        "# Apply preprocessing to text and extract features for each record\n",
        "data['processed_text'] = data['crimeaditionalinfo'].apply(preprocess_text)\n",
        "data[['cyber_terms', 'features', 'entities', 'bigrams', 'trigrams']] = data['crimeaditionalinfo'].apply(\n",
        "    lambda x: pd.Series(extract_features(x))\n",
        ")\n",
        "\n",
        "# Define a Dataset class for the transformer, using only category as label\n",
        "class CrimeCategoryDataset(Dataset):\n",
        "    def __init__(self, texts, labels_category, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels_category = labels_category\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        inputs = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels_category[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Encode categories as numeric labels\n",
        "category_mapping = {label: idx for idx, label in enumerate(data['category'].unique())}\n",
        "data['category'] = data['category'].map(category_mapping)\n",
        "\n",
        "# Split data\n",
        "train_texts, val_texts, train_labels_cat, val_labels_cat = train_test_split(\n",
        "    data['processed_text'].values,\n",
        "    data['category'].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CrimeCategoryDataset(train_texts, train_labels_cat, tokenizer)\n",
        "val_dataset = CrimeCategoryDataset(val_texts, val_labels_cat, tokenizer)\n",
        "\n",
        "# Load pre-trained model and configure it for single-label classification\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased', num_labels=len(category_mapping)\n",
        ").to(device)  # Move model to GPU if available\n",
        "\n",
        "# Training arguments with increased epochs\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,                   # Increased epochs for thorough training\n",
        "    per_device_train_batch_size=32,       # Larger batch size for faster processing\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    save_steps=500,                       # Save checkpoint every 500 steps\n",
        "    save_total_limit=3,                   # Keep only last 3 checkpoints\n",
        "    evaluation_strategy=\"epoch\",          # Evaluate at the end of each epoch\n",
        ")\n",
        "\n",
        "# Define metrics for single-label classification\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train and evaluate\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "\n",
        "# Save the final model and tokenizer\n",
        "model.save_pretrained('./trained_category_model')\n",
        "tokenizer.save_pretrained('./trained_category_model')\n",
        "\n",
        "# Save category mapping to a file for later use\n",
        "import json\n",
        "with open('./trained_category_model/category_mapping.json', 'w') as f:\n",
        "    json.dump(category_mapping, f)\n",
        "\n",
        "# Sample usage of the model for testing:\n",
        "def test_model(text):\n",
        "    text = preprocess_text(text)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n",
        "    output = model(**inputs)\n",
        "    prediction = output.logits.argmax(-1).item()\n",
        "    category = list(category_mapping.keys())[list(category_mapping.values()).index(prediction)]\n",
        "    detected_terms, features, entities, bigrams, trigrams = extract_features(text)\n",
        "\n",
        "    return {\n",
        "        \"category\": category,\n",
        "        \"cyber_terms\": detected_terms,\n",
        "        \"features\": features,\n",
        "        \"entities\": entities,\n",
        "        \"bigrams\": bigrams[:5],  # limiting output for readability\n",
        "        \"trigrams\": trigrams[:5] # limiting output for readability\n",
        "    }\n",
        "\n",
        "# Test example\n",
        "sample_text = \"I received an urgent message saying my bank account was compromised due to phishing.\"\n",
        "print(test_model(sample_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model_path = './trained_category_model'  # Adjust to your saved model path\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Ensure model is in evaluation mode and move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Category mapping (use the same mapping as training)\n",
        "category_mapping = {\n",
        "    0: 'Financial Fraud',\n",
        "    1: 'Identity Theft',\n",
        "    2: 'Phishing Scam',\n",
        "    3: 'Ransomware Attack',\n",
        "    4: 'Social Media Fraud',\n",
        "    5: 'Cryptocurrency Scam',\n",
        "    6: 'Ecommerce Fraud',\n",
        "    7: 'Banking Fraud'\n",
        "}\n",
        "# Reverse the category mapping\n",
        "reverse_category_mapping = {v: k for k, v in category_mapping.items()}\n",
        "\n",
        "# Define additional terms for feature extraction\n",
        "cyber_terms = {\n",
        "    'phishing': 'phish',\n",
        "    'hacked': 'hack',\n",
        "    'scammed': 'scam',\n",
        "    'fraudulent': 'fraud'\n",
        "}\n",
        "\n",
        "features = {\n",
        "    'contains_financial_terms': lambda text: bool(re.search(r'bank|credit|upi|money|payment', text.lower())),\n",
        "    'contains_urgency': lambda text: bool(re.search(r'urgent|immediate|quick|emergency', text.lower())),\n",
        "    'contains_threats': lambda text: bool(re.search(r'threat|hack|stolen|compromise', text.lower())),\n",
        "    'message_length': lambda text: len(text.split()),\n",
        "    'has_numbers': lambda text: bool(re.search(r'\\d', text))\n",
        "}\n",
        "\n",
        "# Victim type detection function\n",
        "def detect_victim_type(text):\n",
        "    victim_keywords = {\n",
        "        'individual': r'\\b(I|me|my|myself|individual|person)\\b',\n",
        "        'company': r'\\b(company|business|organization|firm|corporate)\\b',\n",
        "        'family_member': r'\\b(mother|father|brother|sister|family|son|daughter)\\b',\n",
        "        'friend': r'\\b(friend|acquaintance|buddy|pal)\\b'\n",
        "    }\n",
        "    detected_types = [label for label, pattern in victim_keywords.items() if re.search(pattern, text.lower())]\n",
        "    return detected_types or [\"unknown\"]\n",
        "\n",
        "# Gender detection function\n",
        "def detect_gender(text):\n",
        "    if re.search(r'\\b(she|her|hers)\\b', text.lower()):\n",
        "        return \"female\"\n",
        "    elif re.search(r'\\b(he|his|him)\\b', text.lower()):\n",
        "        return \"male\"\n",
        "    else:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Additional relationship and age detection function\n",
        "def detect_relationship_context(text):\n",
        "    relationships = {\n",
        "        'child': r'\\b(child|children|kid|son|daughter)\\b',\n",
        "        'teenager': r'\\b(teen|teenager|young)\\b',\n",
        "        'adult': r'\\b(adult|man|woman|lady|gentleman)\\b',\n",
        "        'elderly': r'\\b(elderly|senior|old)\\b',\n",
        "    }\n",
        "    detected_relationships = [label for label, pattern in relationships.items() if re.search(pattern, text.lower())]\n",
        "    return detected_relationships or [\"unknown\"]\n",
        "\n",
        "# Bigram and Trigram Analysis function\n",
        "def extract_ngrams(text, n=2):\n",
        "    words = text.lower().split()\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "    return [' '.join(ngram) for ngram in ngrams]\n",
        "\n",
        "# Function for entity recognition and analysis\n",
        "def entity_recognition_analysis(text):\n",
        "    entities = {}\n",
        "    if re.search(r'\\b[A-Z][a-z]+\\b', text):  # Simple detection of names starting with uppercase\n",
        "        entities['person_name'] = True\n",
        "    if re.search(r'\\bLtd|Inc|Corp|Company|Organization\\b', text):\n",
        "        entities['organization'] = True\n",
        "    if re.search(r'\\bStreet|Avenue|District|City\\b', text):\n",
        "        entities['location'] = True\n",
        "    return entities\n",
        "\n",
        "# Define a function to classify a single text, print confidence scores and features\n",
        "def classify_text(text):\n",
        "    # Tokenize input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probs = F.softmax(logits, dim=-1).cpu().numpy().flatten()\n",
        "    top_category_idx = np.argmax(probs)\n",
        "    top_category_label = category_mapping[top_category_idx]\n",
        "    confidence_score = probs[top_category_idx]\n",
        "\n",
        "    # Extract features and print results\n",
        "    extracted_features = {k: v(text) for k, v in features.items()}\n",
        "    extracted_ngrams = extract_ngrams(text, n=2) + extract_ngrams(text, n=3)\n",
        "    entity_info = entity_recognition_analysis(text)\n",
        "\n",
        "    # Additional victim and relationship information\n",
        "    victim_type = detect_victim_type(text)\n",
        "    gender = detect_gender(text)\n",
        "    relationship_context = detect_relationship_context(text)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Predicted Category: {top_category_label}\")\n",
        "    print(f\"Confidence Score: {confidence_score:.2f}\")\n",
        "    print(f\"All Scores: {dict(zip(category_mapping.values(), probs))}\")\n",
        "    print(\"Extracted Features:\", extracted_features)\n",
        "    print(\"Extracted N-Grams:\", extracted_ngrams)\n",
        "    print(\"Entity Recognition Info:\", entity_info)\n",
        "    print(\"Victim Type:\", victim_type)\n",
        "    print(\"Detected Gender:\", gender)\n",
        "    print(\"Relationship Context:\", relationship_context)\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Sample texts for testing\n",
        "sample_texts = [\n",
        "    \"Received a phishing email pretending to be my bank.\",\n",
        "    \"Someone hacked into my social media account and posted inappropriate content.\",\n",
        "    \"I lost money through an online cryptocurrency scam.\",\n",
        "    \"A website I visited contained unlawful content and fake advertisements.\",\n",
        "    \"I was a victim of identity theft through online impersonation.\",\n",
        "    \"I never imagined something like this could happen to me, especially since Im so cautious. But then I started noticing that my online activity felt different, slower even. I cant even describe it well because everything seemed normal, except for the fact that I was constantly getting logged out of my accounts. I dont know how, but it’s like they knew everything I was doing, even though I was trying to be careful. It’s terrifying to think about. The more I tried to fix it, the more problems came up. My work account was notifiction, and then I got locked out of everything. I feel so helpless, like there’s nothing I can do to fix this. No matter how many times I change my password, it’s like someone’s always one step ahead. Even my laptop isn’t working right anymore. It’s so stange how everything just fell apart after that one email.\",\n",
        "    \"time may be incoming call on pooja sharma asked job on bank varienti already search for job so i asked to job site send your whatsapp enter click the link regester your detailafter i enter link my mobile was hacked then enter the link enter our deatil make payment rs enter your debit card detail i submit my card detail after enter payment first amiunt debit rs i have doubt exit the webiste after automatcally enter otp debited my amount after my account balance rs only third person block whatsapp after minutes incoming call on IOB chennai asked your debit oon any scam i to told further block my debit card on IOB bank side thank you\",\n",
        "    \"Respected sir some unknown culprit is cyber bullying me with mail id smggmailcom since with false allegations to our district collector that I am not caring my higher officers allegation of avoiding of holding additional duties allegation of my income sources etc and also continuously cyberbullying me nearly times so please take necessary action on my petition as soon as possible\",\n",
        "    \"While playing online game email id and password he has shared and email was hacked Email id that has been hacked is puneeshagarwalgmailcom I am not able to login now All my contacts data and emails are in this account\"\n",
        "]\n",
        "\n",
        "# Run classification on sample texts\n",
        "for text in sample_texts:\n",
        "    classify_text(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "M0k4wJkvufW_",
        "outputId": "1744eaaa-36e1-4959-9fd4-5cdc60739ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f933c99638d6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistilBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistilBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[0;31m# nn.quant* depends on ao -- so should be after those.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2098\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintrinsic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2099\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantizable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/intrinsic/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Include the subpackages in case user imports from it directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintrinsic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantized\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/intrinsic/quantized/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# to ensure customers can use the module below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# without importing it directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintrinsic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdynamic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintrinsic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/intrinsic/quantized/dynamic/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintrinsic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/intrinsic/quantized/dynamic/modules/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintrinsic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_relu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearReLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m __all__ = [\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"LinearReLU\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintrinsic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearReLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m __all__ = [\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"LinearReLU\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/nn/intrinsic/quantized/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m __all__ = [\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"BNReLU2d\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/nn/intrinsic/quantized/modules/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbn_relu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBNReLU2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBNReLU3d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconv_add\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvAdd2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConvAddReLU2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconv_relu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvReLU1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConvReLU2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConvReLU3d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlinear_relu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearTanh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintrinsic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintrinsic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnnq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/nn/quantized/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaxPool2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mxyU9bbb-qyi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}